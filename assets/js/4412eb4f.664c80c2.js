"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[389],{75540:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"NeoEdge NG4500 Series/Application Guide/Deepseek-r1","title":"DeepSeek-R1 Local Deployment","description":"---","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/1-NeoEdge NG4500 Series/3-Application Guide/1-Deepseek-r1.md","sourceDirName":"1-NeoEdge NG4500 Series/3-Application Guide","slug":"/NeoEdge NG4500 Series/Application Guide/Deepseek-r1","permalink":"/wiki-documents/docs/NeoEdge NG4500 Series/Application Guide/Deepseek-r1","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"JupyterLab","permalink":"/wiki-documents/docs/NeoEdge NG4500 Series/NG4500-CB01 Development Board/Software Guide/Software Frameworks and Tools/jupyter"},"next":{"title":"Pose Estimation","permalink":"/wiki-documents/docs/NeoEdge NG4500 Series/Application Guide/mediapipe"}}');var l=i(74848),s=i(28453);const t={},d="DeepSeek-R1 Local Deployment",o={},c=[{value:"1. Overview",id:"1-overview",level:2},{value:"2. Environment preparation",id:"2-environment-preparation",level:2},{value:"Hardware",id:"hardware",level:3},{value:"Software",id:"software",level:3},{value:"3.Ollama Installation\uff08Inference Engine\uff09",id:"3ollama-installationinference-engine",level:2},{value:"Option A: Native Script Installation",id:"option-a-native-script-installation",level:3},{value:"Option B: Docker Deployment",id:"option-b-docker-deployment",level:3},{value:"Verify Ollama is Running (refer to the code below)",id:"verify-ollama-is-running-refer-to-the-code-below",level:3},{value:"4. Running the DeepSeek-R1",id:"4-running-the-deepseek-r1",level:2},{value:"Getting Start the Model",id:"getting-start-the-model",level:3},{value:"Model Version Comparison",id:"model-version-comparison",level:3},{value:"5. Web Interface (Open WebUI)",id:"5-web-interface-open-webui",level:2},{value:"Install Open WebUI (using Docker)",id:"install-open-webui-using-docker",level:3},{value:"Access the WebUI",id:"access-the-webui",level:3},{value:"6. Performance Optimization",id:"6-performance-optimization",level:2},{value:"7. Troubleshooting",id:"7-troubleshooting",level:2},{value:"8. Appendix",id:"8-appendix",level:2},{value:"Example Directory",id:"example-directory",level:3},{value:"References",id:"references",level:3}];function a(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"deepseek-r1-local-deployment",children:"DeepSeek-R1 Local Deployment"})}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsxs)(n.p,{children:["This guide describes how to locally deploy the ",(0,l.jsx)(n.strong,{children:"DeepSeek-R1 LLMs"})," on ",(0,l.jsx)(n.strong,{children:"NVIDIA Jetson Orin"})," devices using ",(0,l.jsx)(n.strong,{children:"Ollama"}),", a lightweight inference engine, to enable offline AI interaction with a simple and efficient installation process."]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"1-overview",children:"1. Overview"}),"\n",(0,l.jsx)(n.p,{children:"Large language models (LLMs) like DeepSeek-R1 are gradually becoming a core component of edge intelligence applications. Running them directly on Jetson Orin offers key benefits\uff1a"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.strong,{children:"Fully offline operation"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.strong,{children:"Low-latency response"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.strong,{children:"Enhanced data privacy"})}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"This guide including\uff1a"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Environment preparation"}),"\n",(0,l.jsx)(n.li,{children:"Installing Ollama"}),"\n",(0,l.jsx)(n.li,{children:"Running the DeepSeek-R1"}),"\n",(0,l.jsx)(n.li,{children:"(Optional) Using Open WebUI for a web-based interface"}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"2-environment-preparation",children:"2. Environment preparation"}),"\n",(0,l.jsx)(n.h3,{id:"hardware",children:"Hardware"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Component"}),(0,l.jsx)(n.th,{children:"Requirement"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Device"}),(0,l.jsx)(n.td,{children:"Jetson Orin\uff08Nano / NX \uff09"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Memory"}),(0,l.jsx)(n.td,{children:"\u2265 8GB\uff08larger models require more\uff09"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Storage"}),(0,l.jsx)(n.td,{children:"\u2265 10GB\uff08varies by model size\uff09"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"GPU"}),(0,l.jsx)(n.td,{children:"NVIDIA GPU with CUDA support"})]})]})]}),"\n",(0,l.jsx)(n.h3,{id:"software",children:"Software"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Ubuntu 20.04 / 22.04\uff08JetPack 5.1.1+ recommended\uff09"}),"\n",(0,l.jsx)(n.li,{children:"NVIDIA CUDA Toolkit and drivers \uff08included with JetPack\uff09"}),"\n",(0,l.jsx)(n.li,{children:"Docker (optional, for containerized deployment)"}),"\n"]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:["\u2699\ufe0f Tip: Use  ",(0,l.jsx)(n.code,{children:"jetson_clocks"}),"and check ",(0,l.jsx)(n.code,{children:"nvpmodel"})," to enable maximum performance mode for the best inference results."]}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"3ollama-installationinference-engine",children:"3.Ollama Installation\uff08Inference Engine\uff09"}),"\n",(0,l.jsx)(n.h3,{id:"option-a-native-script-installation",children:"Option A: Native Script Installation"}),"\n",(0,l.jsx)(n.p,{children:"Open your terminal or command prompt and run the following command to install the NativeScript CLI."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"curl -fsSL https://ollama.com/install.sh | sh\n"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Installs the Ollama service and CLI tools."}),"\n",(0,l.jsx)(n.li,{children:"Automatically handle dependencies and configure the background service."}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"option-b-docker-deployment",children:"Option B: Docker Deployment"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"sudo docker run --runtime=nvidia --rm --network=host \\\r\n  -v ~/ollama:/ollama \\\r\n  -e OLLAMA_MODELS=/ollama \\\r\n  dustynv/ollama:r36.4.0\n"})}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsx)(n.p,{children:"\ud83e\udde9 The Docker version is maintained by the NVIDIA community (dustynv) and optimized for Jetson."}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"verify-ollama-is-running-refer-to-the-code-below",children:"Verify Ollama is Running (refer to the code below)"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"ss -tuln | grep 11434\n"})}),"\n",(0,l.jsx)(n.p,{children:"Expected output:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"LISTEN 0 128 127.0.0.1:11434 ...\n"})}),"\n",(0,l.jsxs)(n.p,{children:["If port ",(0,l.jsx)(n.code,{children:"11434"}),"  is listening, the Ollama service has started successfully."]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"4-running-the-deepseek-r1",children:"4. Running the DeepSeek-R1"}),"\n",(0,l.jsx)(n.h3,{id:"getting-start-the-model",children:"Getting Start the Model"}),"\n",(0,l.jsx)(n.p,{children:"To run the 1.5B parameter version\uff1a"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"ollama run deepseek-r1:1.5b\n"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Ollama will automatically download the model if it is not cached locally."}),"\n",(0,l.jsx)(n.li,{children:"Starts an interactive conversation in the command line."}),"\n"]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:["\ud83d\udca1 Depending on your hardware capability, you can replace ",(0,l.jsx)(n.code,{children:"1.5b"})," with ",(0,l.jsx)(n.code,{children:"8b"}),"\u3001",(0,l.jsx)(n.code,{children:"14b"})," ,etc."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"model-version-comparison",children:"Model Version Comparison"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Version"}),(0,l.jsx)(n.th,{children:"Memory Requirement"}),(0,l.jsx)(n.th,{children:"Notes"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"1.5B"}),(0,l.jsx)(n.td,{children:"~6\u20138 GB"}),(0,l.jsx)(n.td,{children:"Suitable for Orin Nano/NX"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"8B+"}),(0,l.jsx)(n.td,{children:"\u2265 16 GB"}),(0,l.jsx)(n.td,{children:"Requires AGX Orin"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"70B"}),(0,l.jsx)(n.td,{children:"\ud83d\udeab"}),(0,l.jsx)(n.td,{children:"Not supported on Jetson"})]})]})]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"5-web-interface-open-webui",children:"5. Web Interface (Open WebUI)"}),"\n",(0,l.jsxs)(n.p,{children:["Open WebUI provides a user-friendly browser-based chat interface.\r\n",(0,l.jsx)(n.img,{alt:"open_webui",src:i(6363).A+"",width:"1920",height:"1080"})]}),"\n",(0,l.jsx)(n.h3,{id:"install-open-webui-using-docker",children:"Install Open WebUI (using Docker)"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"sudo docker run -d --network=host \\\r\n  -v ${HOME}/open-webui:/app/backend/data \\\r\n  -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \\\r\n  --name open-webui \\\r\n  --restart always \\\r\n  ghcr.io/open-webui/open-webui:main\n"})}),"\n",(0,l.jsx)(n.h3,{id:"access-the-webui",children:"Access the WebUI"}),"\n",(0,l.jsx)(n.p,{children:"Visit your browser with\uff1a"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"http://localhost:3000/\n"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"You can interact with the DeepSeek-R1 model graphically"}),"\n",(0,l.jsx)(n.li,{children:"View conversation history, and review model responses directly in the browser."}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"6-performance-optimization",children:"6. Performance Optimization"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Optimization Area"}),(0,l.jsx)(n.th,{children:"Description"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Memory Usage"}),(0,l.jsx)(n.td,{children:"Use a smaller model (e.g., 1.5B) or enable swap"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Jetson Performance"}),(0,l.jsxs)(n.td,{children:["Enable ",(0,l.jsx)(n.code,{children:"MAXN"})," and run ",(0,l.jsx)(n.code,{children:"jetson_clocks"})]})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Model Caching"}),(0,l.jsxs)(n.td,{children:["Ensure sufficient space in the ",(0,l.jsx)(n.code,{children:"~/ollama"})," directory"]})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Runtime Monitoring"}),(0,l.jsxs)(n.td,{children:["Use ",(0,l.jsx)(n.code,{children:"htop"}),"or",(0,l.jsx)(n.code,{children:"tegrastats"})," to monitor system load"]})]})]})]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsx)(n.p,{children:"\ud83d\udcc9 The initial model load may take about 30 seconds to 1 minute; subsequent runs will be faster thanks to caching."}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"7-troubleshooting",children:"7. Troubleshooting"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Issue"}),(0,l.jsx)(n.th,{children:"Solution"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Port 11434 not listening"}),(0,l.jsx)(n.td,{children:"Restart Ollama or check Docker container status"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Model fails to load"}),(0,l.jsx)(n.td,{children:"Insufficient memory; try using a smaller version (e.g., 1.5B)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Cannot access Web UI"}),(0,l.jsx)(n.td,{children:"Verify Docker is running and connected to the host network"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Ollama command not found"}),(0,l.jsxs)(n.td,{children:["Re-run the installation script or add it to your  ",(0,l.jsx)(n.code,{children:"$PATH"})]})]})]})]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"8-appendix",children:"8. Appendix"}),"\n",(0,l.jsx)(n.h3,{id:"example-directory",children:"Example Directory"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"~/ollama/                # Model cache directory  \r\n~/open-webui/            # WebUI persistent data \n"})}),"\n",(0,l.jsx)(n.h3,{id:"references",children:"References"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://huggingface.co/deepseek-ai",children:"DeepSeek-R1  Model on HuggingFace"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://ollama.com/",children:"Ollama  Official Documentation"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://github.com/open-webui/open-webui",children:"Open WebUI GitHub"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://forums.developer.nvidia.com/",children:"NVIDIA Jetson Developer Forum"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(a,{...e})}):a(e)}},6363:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/open_webui-d3952c8561c4808c1d447fc061c71174.gif"},28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>d});var r=i(96540);const l={},s=r.createContext(l);function t(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:t(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);